\subsection{Popular methods}
Since we can transform the problem of optimal control into a two-point 
boundary ODE problem, the methods designed for this sort of problem are 
applicable see \cite{Keller1976, Ascher1987} for classic references. In this 
line \cite{Caetano2001,Yan2008} use multiple shooting methods to solve the 
resulting extended two-value boundary ode.
\paragraph{Multiple shooting method}
Roughly speaking, the multiple shooting method follows the next algorithm.
Consider the controlled dynamics and corresponding adjoint equations given by
\begin{equation}
\label{eqn:extended_tpvbp}
\begin{aligned}
\dot{x}(t) &= 
f(x(t), u(t)), \qquad x(0)=x_0 \\
\dot{\lambda}(t) &=
-\mathcal{H}_x(t,x(t),u(t),\lambda_0,\lambda(t))^\top, \quad 
\lambda(T)=0.
\end{aligned}
\end{equation}
Given  partition of the interval $[0, T]$ with uniform step $h$,
$$
\tau_h^n:= \{t_k = kh, \ k=0,\dots n\}
$$
\begin{enumerate}[Step 1]
  \item
  Choose $y_i := [x(t_i ), \lambda(t_i )], \quad i = 1,\dots, n$. 
  \item 
  Integrate \eqref{eqn:extended_tpvbp} for each time interval $[t_i , 
  t_{i+1})$ 
  using $y_i$ as the initial conditions and
  obtaining $y(t_{i−1}) = [x(t_{i−1}), \lambda(t_{i−1})]$, 
  $i=2, \dots, n$.
  \item
  Compute $y^{new}:= y_i − y(t_i )$.  
  If $y^{new}$ is sufficiently close to a minimum value, then stop; 
  Else actualize the initial conditions $y_i$ 
  for the next iteration (using, for instance, the Newton-Raphson 
  algorithm) and go to  Step 2.   
\end{enumerate}  

\begin{algorithm}
  \caption{Multi shooting method } \label{alg:multishooting}
  \begin{flushleft}
    \hspace*{\algorithmicindent} \textbf{Input:} 
    $t_0, T, x_0, h, \text{tol}, \lambda_{f}$ \\
    \hspace*{\algorithmicindent} \textbf{Output:} 
    $x^*, u^*, \lambda$
  \end{flushleft}
  \begin{algorithmic}
    \Procedure{Multishooting}{$g,\lambda_{\text{function}}, 
      u, x_0, 
      \lambda_f, h, n_{max}$} 
    \While{$ \epsilon > \text{tol}  $}
    \State $u_{\text{old}} \gets u$ 
    \State $x_{\text{old}} \gets x$ 
    \State $\lambda_{\text{old}} \gets \lambda $
    \State $ 
    [x, \lambda] \gets$
    \Call{quadrature}{$f,\mathcal{H}_{x}, u, x_0, h$}
    \State $u_1 \gets$ 
    \Call{optimality\_condition}{$u, x, \lambda$}
    %
    \State 
    $u \gets \alpha u_1 + (1-\alpha)u_{old}, 
    \qquad \alpha \in [0, 1]$
    \Comment{convex combination}
    \State 
    $\epsilon_u \gets \displaystyle 
    \frac{||u - u_{\text{old}}||}{||u||}$
    \State 
    $\epsilon_x \gets \displaystyle 
    \frac{||x - x_{\text{old}}||}{||x||}$
    \Comment{relative error}
    \State 
    $\epsilon_{\lambda} \gets \displaystyle 
    \frac{||\lambda - 
      \lambda_{\text{old}}||}{||\lambda||}$
    \State 
    $\epsilon \gets 
    \max{ 
      \{ \epsilon_u, \epsilon_x, \epsilon_{\lambda} \}
    }$
    \EndWhile\label{}
    \State \textbf{return} $ x^*, u^*, \lambda$
    \Comment{Optimal pair}
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\paragraph{The Forward-Backward-Sweep}
\cite{hackbusch1978numerical}
\cite*{Pesch1989} , [Saúl]
\paragraph{Genetic Algorithms}
[Frank]
\begin{algorithm}
  \caption{Forward Backward Sweep } \label{alg:forward_backward_sweep}
  \begin{flushleft}
    \hspace*{\algorithmicindent} \textbf{Input:} 
    $t_0, t_f, x_0,h, \text{tol}, \lambda_{f}$ \\
    \hspace*{\algorithmicindent} \textbf{Output:} 
    $x^*, u^*, \lambda$
  \end{flushleft}
  \begin{algorithmic}
    \Procedure{Forward backward sweep}{$g,\lambda_{\text{function}}, 
      u, x_0, 
      \lambda_f, h, n_{max}$} 
    \While{$ \epsilon > \text{tol}  $}
    \State $u_{\text{old}} \gets u$ 
    \State $x_{\text{old}} \gets x$ 
    \State $ x \gets$
    \Call{runge\_kutta\_forward}{$g, u, x_0, h$}
    \State $\lambda_{\text{old}} \gets \lambda $
    \State $\lambda \gets$ 
    \Call{runge\_kutta\_backward}{%
      $\lambda_{\text{function}}, x, \lambda_f, h$
    }
    \State $u_1 \gets$ 
    \Call{optimality\_condition}{$u, x, \lambda$}
    %
    \State 
    $u \gets \alpha u_1 + (1-\alpha)u_{old}, 
    \qquad \alpha \in [0, 1]$
    \Comment{convex combination}
    \State 
    $\epsilon_u \gets \displaystyle 
    \frac{||u - u_{\text{old}}||}{||u||}$
    \State 
    $\epsilon_x \gets \displaystyle 
    \frac{||x - x_{\text{old}}||}{||x||}$
    \Comment{relative error}
    \State 
    $\epsilon_{\lambda} \gets \displaystyle 
    \frac{||\lambda - \lambda_{\text{old}}||}{||\lambda||}$
    \State 
    $\epsilon \gets 
    \max{ 
      \{ \epsilon_u, \epsilon_x, \epsilon_{\lambda} \}
    }$
    \EndWhile\label{}
    \State \textbf{return} $ x^*, u^*, \lambda$
    \Comment{Optimal pair}
    \EndProcedure
  \end{algorithmic}
\end{algorithm}
