\paragraph{\it Uniqueness of optimal policy}
The proof of the uniqueness of the state path $X_u$, given a policy $u$, is
fairly standard (Theorem \ref{ExAdmisPair}). However, the uniqueness of an 
{\it optimal policy} is not trivial and it can be established on some small 
enough interval; see, for instance, \cite{GaffSchaefer09} and the references 
therein.  
\medskip

\paragraph{\it Maximum principle vs. Dynamic programming} 
  The same approach is followed in almost all the related literature on optimal
control of epidemics/diseases. As an alternative, the so-called Dynamic
programming approach can be used to analyze this kind of problems. With the
Maximum principle we need to solve a system of ordinary differential equations
(ODEs) whereas in Dynamic programming a partial differential equation (PDE)
arises. In addition, both approaches involve an optimization problem. The
Maximum principle is mostly used because there are plenty of methods to
numerically solve ODEs.
\medskip
\paragraph{\it Numerical Issues}

\medskip
\paragraph{\it Evolutionary algorithms}
Evolutionary algorithms are a kind of heuristics algorithms well 
suited for global optimization. Such algorithms emulate natural
evolution introducing operators for mutation ($\mathbf{M}$), crossover 
($\mathbf{C}$) and selection ($\mathbf{S}$). One of the earliest works
on evolutionary methods was developed by George E. P. Box \cite{Box1957}.
Nevertheless, it can be said tat the first evolutionary algorithm at
least as they are known today, was the so called Genetic Algorithm (GA) 
introduced by J. H. Hollad \cite{JHH1975}. Since then many variations of
evolutionary algorithms have been developed being Differential Evolution (DE),
introduced by Price and Storn \cite{Storn1997}, one of the simplest yet efficient and 
effective optimization algorithms. Algorithm \ref{alg:DE1} shows the 
general form of Evolutionary Algorithms for optimizing the $fob$ 
objective function. There, an initial population $Y$ of size $N_p$, 
generated in the search space $\mathcal{V}$ by the operator $\mathbf{Y}_0$;
is subject to the evolutionary process until certain stopping criterion is 
met. Then the best individual ($\mathbf{y}_{best}$), i.e., the individual who
optimizes $fob$, is selected by introducing the operator $\mathbf{Best}$. 
In Algorithm \ref{alg:DE1} the variable $M$ stores a mutated population, 
the variable $C$ stores the results of the crossover operator. The selection
operator selects from $M$ and $C$ the individuals which will conform the new
population $Y$. This selection is based in some criteria usually dictated by the
objective function. For instance, if $\mathbf{y}$ is an element of $Y$
and $\mathbf{c}$ an element of $C$, a common criterion for minimization is 
to select $\mathbf{y}$ if $fob(\mathbf{y})<fob(\mathbf{c})$.
A detailed explanation for constructing the main operators 
$\mathbf{M}$, $\mathbf{C}$ and $\mathbf{S}$, can be found in
\cite{Bagchi1999} for GA and in \cite{Price_Storn2005} for the DE
algorithm.
%
\begin{algorithm}[H]
\caption{Evolutionary Algorithms}
\label{alg:DE1}
\begin{algorithmic}
% \small
\State $Y$ $\leftarrow$ $\mathbf{Y}_0(Np,\mathcal{V})$
\While{(the stopping criterion has not been met)}
\State $M$ $\leftarrow$  $\mathbf{M}(Y)$
\State $C$ $\leftarrow$  $\mathbf{C}(Y,C)$
\State $Y$ $\leftarrow$  $\mathbf{S}(Y,C,fob)$ 
\EndWhile
\State $\mathbf{y}_{best} \leftarrow \mathbf{Best}(Y, fob)$
\end{algorithmic}
\end{algorithm}%


Falta....